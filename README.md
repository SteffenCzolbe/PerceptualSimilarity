# A Loss Function for Generative Neural Networks Based on Watsonâ€™s Perceptual Model

Steffen Czolbe, Oswin Krause, Igemar Cox, Christian Igel - NeurIPS 2020

[[Paper]](https://arxiv.org/abs/2006.15057) [[Video]](https://youtu.be/qPmHQbR4DeI) [[Poster]](img/WatsonPoster.pdf)

> **Note:** The initialy published version of this repository contained inconsistent data normalization across experiments. This has since been fixed, and we are currently re-tuning hyperparameters and re-running experiments. The mostly finsihed updated can be viewed in the branch [normalization_fix](https://github.com/SteffenCzolbe/PerceptualSimilarity/tree/normalization_fix). We re-generated all the figures in the paper, and observe overall very similar results.

[![Video](https://img.youtube.com/vi/qPmHQbR4DeI/hqdefault.jpg)](https://youtu.be/qPmHQbR4DeI)

This repository contains the similarity metrics designed and evaluated in the [paper](https://arxiv.org/abs/2006.15057), and instructions and code to re-run the experiments. Implementation in the deep-learning framework PyTorch. Code supplied in Python 3 files and Jupyter Notebooks.

# Use the similarity metrics

The presented similarity metrics can be included in your projects by importing the `LossProvider`. It makes all pre-trained similarity metrics accessible. The example below shows how to build the `Watson-DFT` metric, and loads the weights tuned on the 2AFC dataset. The input for all loss functions is expected to be normalized to a -1..1 interval.

```python
from loss.loss_provider import LossProvider

provider = LossProvider()
loss_function = provider.get_loss_function('Watson-DFT', colorspace='RGB', pretrained=True, reduction='sum')

import torch
img0 = torch.zeros(1,3,64,64)
img1 = torch.zeros(1,3,64,64)
loss = loss_function(img0, img1)
```

Parameters:

- The first parameter defines the loss metric. Implemented metrics are `'L1', 'L2', 'SSIM', 'Adaptive', 'Watson-DCT', 'Watson-DFT', 'Deeploss-VGG', 'Deeploss-Squeeze'`.
- Keyword argument `colorspace` defines the color representation and dimensionality of the input. Default is the three-channel `'RGB'` model. Mono-channel greyscale representation can be used py passing `'LA'`.
- Keyword argument `pretrained`. If `True` (default), the weights pre-trained on the 2AFC task are loaded.
- Keyword argument `reduction`, with behaviour according to PyTorch guideline. Default value is `reduction='sum'`. All metrics further support option `reduction='none'`.
- Keyword argument `deterministic`. Determines the shifting behaviour of metrics `'Watson-DCT'` and `'Watson-DFT'`. The shifts make the metric non-deterministic, but lead to faster convergence and better results. Though in some cases we might prefer a deterministic behaviour. Default `deterministic=False`.
- Keyword argument `image_size`. Only required for `'Adaptive'`-Loss, as the implementation provided by the authors requires the input size. Example: `image_size=(3, 64, 64)`.

# Experiments

**WARNING:** This part of the codebase is extremely unorganized. It was created as part of a master thesis, whithout much experience on how to write or maintain code for research. There is no neat `run_all_experiments_and_make_me_beacon.sh` script here. We provide the code as is, as we believe it is still helpfull for those willing and determined enough to work with it. You have been warned.

## Dependencies

The project is implemented in python. Dependencies can be installed via

```bash
$ pip3 install -r requirements.txt
```

Alternative: It is recommended to use a virtual environment. This will keep changes to your local python installation contained. First, we set up a virtual environment called `venv-perceptual-sim`:

```bash
sudo apt-get install python3-pip
python3 -m pip install virtualenv
python3 -m virtualenv venv-perceptual-sim
```

Next, we activate it and install our dependencies

```bash
source venv-perceptual-sim/bin/activate
pip3 install -r requirements.txt
```

If you want to use this kernel within jupyter notebooks, you also need to register it as an ipython kernel.

```bash
ipython kernel install --user --name=venv-perceptual-sim
```

## Download Data

We use three datasets: MNIST, celebA and 2AFC. Execute the following commands to download all three datasets:

```bash
cd ./src/
PSHOME=$(pwd)
cd $PSHOME/datasets/MNIST
python3 mnist_download.py
cd $PSHOME/datasets/celebA
python3 celebA_download.py
cd $PSHOME/perceptual_sim_training
./scripts/download_dataset.sh
```

## Tune on the 2AFC Set

The tuning and evaluation on the 2AFC dataset is located in directory `src/perceptual-sim-training`. The code is adapted from Zhang et. al. [2018], and includes adjustments to train and evaluate our Modified Watson Metrics.

### Train

To train the metrics, we execute the provided training script.

```bash
cd $PSHOME/perceptual_sim_training
./scripts/train.sh
```

### Evaluate

The evaluation of the metrics performance on the validation section of the 2AFC dataset is performed in jupyter notebooks. We start jupyter:

```bash
jupyter notebook
```

Wait for jupyter to open in your browser. Navigate to `src/perceptual-sim-training/eval.ipynb`. Open the notebook, and select your virtual environment, and execute all cells. The 2AFC scores are written to a file.

To generate the plots from the paper, navigate to the notebook `src/perceptual-sim-training/eval-plot.ipynb` and execute it.

## Transition weights to the LossProvider

To transition loss functions tuned on the 2AFC dataset to the loss provider, we open the jupyter notebook `src/perceptual-sim-training/transition_weights.ipynb` and execute it. The lates model checkpoints from the 2AFC experiment are extracted, renamed and saved into the he 2AFC dataset to the loss provider, we open the jupyter notebook `src/loss/weights/` directory.

## Train VAEs

The VAEs presented in the paper can be retrained with code from directories `src/vae/mnist` and `src/vae/celebA`. Each directory contains 5 Files:

- the implementation of the model in `celebA_vae.py` or `mnist_vae.py`
- a notebook `train.ipynb`, containing code to train the model. Loss function and Hyperparameters are defined in the last cell. During training, model checkpoints and samples are saved in the `results` directory.
- a directory `results`, in which models and sample images are saved during training
- a notebook `evaluation.ipynb`, containg code to generate the comparison images shown in the paper.
- a directory `comparison`, in which comparison images are saved

## Measure Resource Requirements

The resource experiment is located in `src/runtime/runtime_experiment.py`. It will run all loss functions multiple times, and print out the averaged measurments at the end. Results are also saved as as a pickle file.

# Thanks

We thank Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang for their Two Alternative Forced Choice (2AFC) dataset, and implementation of the perceptual tuning code. We adapted large parts of their [code](https://github.com/richzhang/PerceptualSimilarity) for this project.

We also thank Jonathan T. Barron for providing the implementation f his [General and Adaptive Robust Loss Function](https://github.com/jonbarron/robust_loss_pytorch), which we incuded in this project.
